
@online{sarkar_deep_2018,
	title = {Deep Transfer Learning for Natural Language Processing — Text Classification with Universal…},
	url = {https://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9},
	abstract = {A Guide to Demystifying Universal Sentence Encoders},
	author = {Sarkar, Dipanjan ({DJ})},
	urldate = {2021-07-25},
	date = {2018-12-13},
}

@online{noauthor_guide_2019,
	title = {Guide to Transfer Learning with Real-World Applications in Deep Learning},
	url = {https://thirdeyedata.io/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning/},
	abstract = {A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning". Visit Now to know more.},
	titleaddon = {{ThirdEye} Data},
	urldate = {2021-07-27},
	date = {2019-05-30},
	langid = {american},
	note = {Section: Deep Learning},
	file = {Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\AI6FR8Y5\\a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learni.html:text/html},
}

@article{smetanin_applications_2020,
	title = {The Applications of Sentiment Analysis for Russian Language Texts: Current Challenges and Future Perspectives},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3002215},
	shorttitle = {The Applications of Sentiment Analysis for Russian Language Texts},
	abstract = {Sentiment analysis has become a powerful tool in processing and analysing expressed opinions on a large scale. While the application of sentiment analysis on English-language content has been widely examined, the applications on the Russian language remains not as well-studied. In this survey, we comprehensively reviewed the applications of sentiment analysis of Russian-language content and identified current challenges and future research directions. In contrast with previous surveys, we targeted the applications of sentiment analysis rather than existing sentiment analysis approaches and their classification quality. We synthesised and systematically characterised existing applied sentiment analysis studies by their source of analysed data, purpose, employed sentiment analysis approach, and primary outcomes and limitations. We presented a research agenda to improve the quality of the applied sentiment analysis studies and to expand the existing research base to new directions. Additionally, to help scholars selecting an appropriate training dataset, we performed an additional literature review and identified publicly available sentiment datasets of Russian-language texts.},
	pages = {110693--110719},
	journaltitle = {{IEEE} Access},
	author = {Smetanin, Sergey},
	date = {2020},
	note = {Conference Name: {IEEE} Access},
	keywords = {applications of sentiment analysis, Classification, computational linguistics, Indexes, machine learning, Machine learning, Monitoring, public opinion, Russian-language texts, sentiment analysis, Sentiment analysis, Social networking (online), Task analysis, Training},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\IKPC52PV\\Smetanin - 2020 - The Applications of Sentiment Analysis for Russian.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\GNPYU2CC\\9117010.html:text/html},
}

@article{lin_improving_2017,
	title = {Improving {EEG}-Based Emotion Classification Using Conditional Transfer Learning},
	volume = {11},
	issn = {1662-5161},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5486154/},
	doi = {10.3389/fnhum.2017.00334},
	abstract = {To overcome the individual differences, an accurate electroencephalogram ({EEG})-based emotion-classification system requires a considerable amount of ecological calibration data for each individual, which is labor-intensive and time-consuming. Transfer learning ({TL}) has drawn increasing attention in the field of {EEG} signal mining in recent years. The {TL} leverages existing data collected from other people to build a model for a new individual with little calibration data. However, brute-force transfer to an individual (i.e., blindly leveraged the labeled data from others) may lead to a negative transfer that degrades performance rather than improving it. This study thus proposed a conditional {TL} ({cTL}) framework to facilitate a positive transfer (improving subject-specific performance without increasing the labeled data) for each individual. The {cTL} first assesses an individual’s transferability for positive transfer and then selectively leverages the data from others with comparable feature spaces. The empirical results showed that among 26 individuals, the proposed {cTL} framework identified 16 and 14 transferable individuals who could benefit from the data from others for emotion valence and arousal classification, respectively. These transferable individuals could then leverage the data from 18 and 12 individuals who had similar {EEG} signatures to attain maximal {TL} improvements in valence- and arousal-classification accuracy. The {cTL} improved the overall classification performance of 26 individuals by {\textasciitilde}15\% for valence categorization and {\textasciitilde}12\% for arousal counterpart, as compared to their default performance based solely on the subject-specific data. This study evidently demonstrated the feasibility of the proposed {cTL} framework for improving an individual’s default emotion-classification performance given a data repository. The {cTL} framework may shed light on the development of a robust emotion-classification model using fewer labeled subject-specific data toward a real-life affective brain-computer interface ({ABCI}).},
	pages = {334},
	journaltitle = {Frontiers in Human Neuroscience},
	shortjournal = {Front Hum Neurosci},
	author = {Lin, Yuan-Pin and Jung, Tzyy-Ping},
	urldate = {2021-07-27},
	date = {2017-06-27},
	pmid = {28701938},
	pmcid = {PMC5486154},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\46F5PKSX\\Lin and Jung - 2017 - Improving EEG-Based Emotion Classification Using C.pdf:application/pdf},
}

@online{asgarian_transfer_2019,
	title = {Transfer Learning — part 1},
	url = {https://medium.com/georgian-impact-blog/transfer-learning-part-1-ed0c174ad6e7},
	abstract = {In this post, we first give a brief overview of Transfer Learning and then we discuss some of its application in the industry.},
	titleaddon = {Georgian Impact Blog},
	author = {asgarian, azin},
	urldate = {2021-07-29},
	date = {2019-08-13},
	langid = {english},
}

@online{noauthor_transfer_2017,
	title = {Transfer Learning - Machine Learning's Next Frontier},
	url = {https://ruder.io/transfer-learning/},
	abstract = {Deep learning models excel at learning from a large number of labeled examples, but typically do not generalize to conditions not seen during training. This post gives an overview of transfer learning, motivates why it warrants our application, and discusses practical applications and methods.},
	titleaddon = {Sebastian Ruder},
	urldate = {2021-07-27},
	date = {2017-03-21},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\28Z69X7D\\transfer-learning.html:text/html},
}

@article{liu_survey_2019,
	title = {A Survey of Sentiment Analysis Based on Transfer Learning},
	volume = {{PP}},
	doi = {10.1109/ACCESS.2019.2925059},
	abstract = {With the rapid development of the Internet industry, sentiment analysis has grown into one of the popular areas of natural language processing ({NLP}). Through it, the implicit emotion in text can be effectively mined, which can help enterprises or organizations to make effective decision, and the explosive growth of data undoubtedly brings more opportunities and challenges to sentiment analysis. At the same time, transfer learning has emerged as a new machine learning technique that uses existing knowledge to solve different domain problems and produces state-of-the-art prediction results. Many scholars apply transfer learning to the field of sentiment analysis. This survey summarizes the relevant research results of sentiment analysis in recent years and focuses on the algorithms and applications of transfer learning in sentiment analysis, and we look forward to the development trend of sentiment analysis.},
	pages = {1--1},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Liu, Remi and {SHI}, Yuqian and {JI}, Changjiang and {JIA}, Ming},
	date = {2019-06-26},
	file = {Full Text:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\7B3AGFPB\\Liu et al. - 2019 - A Survey of Sentiment Analysis Based on Transfer L.pdf:application/pdf},
}

@article{howard_universal_2018,
	title = {Universal Language Model Fine-tuning for Text Classification},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in {NLP} still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning ({ULMFiT}), an effective transfer learning method that can be applied to any task in {NLP}, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	journaltitle = {{arXiv}:1801.06146 [cs, stat]},
	author = {Howard, Jeremy and Ruder, Sebastian},
	urldate = {2021-07-27},
	date = {2018-05-23},
	eprinttype = {arxiv},
	eprint = {1801.06146},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\D6BT5IRL\\Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\25XQ3ZII\\1801.html:text/html},
}

@online{ganesh_pre-trained_2019,
	title = {Pre-trained Language Models : Simplified},
	url = {https://towardsdatascience.com/pre-trained-language-models-simplified-b8ec80c62217},
	shorttitle = {Pre-trained Language Models},
	abstract = {Sesame street of the {NLP} world},
	titleaddon = {Medium},
	author = {Ganesh, Prakhar},
	urldate = {2021-07-27},
	date = {2019-12-17},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\ZEZRBVLZ\\pre-trained-language-models-simplified-b8ec80c62217.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2021-07-27},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\LSLD22PC\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\TN3CDT2F\\1810.html:text/html},
}

@inproceedings{williams_broad-coverage_2018,
	location = {New Orleans, Louisiana},
	title = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
	url = {http://aclweb.org/anthology/N18-1101},
	doi = {10.18653/v1/N18-1101},
	abstract = {This paper introduces the Multi-Genre Natural Language Inference ({MultiNLI}) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difﬁculty. {MultiNLI} accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford {NLI} corpus shows that it represents a substantially more difﬁcult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.},
	eventtitle = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)},
	pages = {1112--1122},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 1 (Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel},
	urldate = {2021-07-27},
	date = {2018},
	langid = {english},
	file = {Williams et al. - 2018 - A Broad-Coverage Challenge Corpus for Sentence Und.pdf:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\EU6L5U67\\Williams et al. - 2018 - A Broad-Coverage Challenge Corpus for Sentence Und.pdf:application/pdf},
}

@inproceedings{wang_glue_2018,
	location = {Brussels, Belgium},
	title = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	url = {http://aclweb.org/anthology/W18-5446},
	doi = {10.18653/v1/W18-5446},
	shorttitle = {{GLUE}},
	abstract = {For natural language understanding ({NLU}) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation ({GLUE}) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing {NLU} tasks. By including tasks with limited training data, {GLUE} is designed to favor and encourage models that share general linguistic knowledge across tasks. {GLUE} also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general {NLU} systems.},
	eventtitle = {Proceedings of the 2018 {EMNLP} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	pages = {353--355},
	booktitle = {Proceedings of the 2018 {EMNLP} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	urldate = {2021-07-27},
	date = {2018},
	langid = {english},
	file = {Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\E77FAASE\\Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:application/pdf},
}

@article{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {http://arxiv.org/abs/1606.05250},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	journaltitle = {{arXiv}:1606.05250 [cs]},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2021-07-27},
	date = {2016-10-10},
	eprinttype = {arxiv},
	eprint = {1606.05250},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\32DDCMGK\\Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\5LZZ2SRR\\1606.html:text/html},
}

@article{joshi_fall-back_nodate,
	title = {A Fall-back Strategy for Sentiment Analysis in Hindi: a Case Study},
	abstract = {Sentiment Analysis ({SA}) research has gained tremendous momentum in recent times. However, there has been little work in this area for an Indian language. We propose in this paper a fall-back strategy to do sentiment analysis for Hindi documents, a problem on which, to the best of our knowledge, no work has been done until now. (A) First of all, we study three approaches to perform {SA} in Hindi. We have developed a sentiment annotated corpora in the Hindi movie review domain. The ﬁrst of our approaches involves training a classiﬁer on this annotated Hindi corpus and using it to classify a new Hindi document. (B) In the second approach, we translate the given document into English and use a classiﬁer trained on standard English movie reviews to classify the document. (C) In the third approach, we develop a lexical resource called Hindi-{SentiWordNet} (H-{SWN}) and implement a majority score based strategy to classify the given document.},
	pages = {6},
	author = {Joshi, Aditya},
	langid = {english},
	file = {Joshi - A Fall-back Strategy for Sentiment Analysis in Hin.pdf:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\RJNYV6WW\\Joshi - A Fall-back Strategy for Sentiment Analysis in Hin.pdf:application/pdf},
}

@misc{kumar_bhaav_2019,
	title = {{BHAAV} - A Text Corpus for Emotion Analysis from Hindi Stories},
	rights = {Restricted Access},
	url = {https://zenodo.org/record/3457467},
	abstract = {In this paper, we introduce the first and largest Hindi text corpus, named {BHAAV} , which means emotions in Hindi, for analyzing emotions that a writer expresses through his characters in a story, as perceived by a narrator/reader. The corpus consists of 20,304 sentences collected from 230 different short stories spanning across 18 genres such as (Inspirational) and (Mystery). Each sentence has been annotated into one of the five emotion categories (anger, joy, suspense, sad, and neutral), by three native Hindi speakers with at least ten years of formal education in Hindi. We also discuss challenges in the annotation of low resource languages such as Hindi, and discuss the scope of the proposed corpus along with its possible uses. We also provide a detailed analysis of the dataset and train strong baseline classifiers reporting their performances.},
	publisher = {Zenodo},
	author = {Kumar, Yaman and Debanjan Mahata and Aggarwal, Sagar and Anmol Chugh and Rajat Maheshwari and Shah, Rajiv Ratn},
	urldate = {2021-07-28},
	date = {2019-09-22},
	langid = {english},
	doi = {10.5281/ZENODO.3457467},
	note = {Type: dataset},
	file = {Kumar et al. - 2019 - BHAAV - A Text Corpus for Emotion Analysis f.pdf:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\6XPUXZZY\\Kumar et al. - 2019 - BHAAV- A Text Corpus for Emotion Analysis f.pdf:application/pdf},
}

@software{sinha_sid573hindi_sentiment_analysis_2021,
	title = {sid573/Hindi\_Sentiment\_Analysis},
	url = {https://github.com/sid573/Hindi_Sentiment_Analysis},
	abstract = {Model to predict the sentiment of Hindi sentences developed this model during my 2nd-year Internship @ algo8.ai},
	author = {Sinha, Siddhant},
	urldate = {2021-07-28},
	date = {2021-05-13},
	note = {original-date: 2019-06-11T09:27:27Z},
	keywords = {fastai, nlp},
}

@online{noauthor_ml_2018,
	title = {{ML} {\textbar} Label Encoding of datasets in Python},
	url = {https://www.geeksforgeeks.org/ml-label-encoding-of-datasets-in-python/},
	abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
	titleaddon = {{GeeksforGeeks}},
	urldate = {2021-07-28},
	date = {2018-10-15},
	langid = {english},
	note = {Section: Machine Learning},
	file = {Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\HZNY2PQW\\ml-label-encoding-of-datasets-in-python.html:text/html},
}

@online{noauthor_tokenizer_nodate,
	title = {Tokenizer},
	url = {https://huggingface.co/transformers/main_classes/main_classes/tokenizer.html},
	abstract = {A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two...},
	urldate = {2021-07-28},
	langid = {english},
}

@online{noauthor_bert_nodate,
	title = {{BERT}},
	url = {https://huggingface.co/transformers/model_doc/model_doc/bert.html},
	abstract = {Overview: The {BERT} model was proposed in {BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Ke...},
	urldate = {2021-07-28},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\5RUFHFDM\\bert.html:text/html},
}


@online{noauthor_pytorch_nodate,
	title = {{PyTorch}},
	url = {https://www.pytorch.org},
	abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
	urldate = {2021-07-28},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\NJVMDCAN\\pytorch.org.html:text/html},
}

@article{srivastava_dropout_nodate,
	title = {Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	pages = {30},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	langid = {english},
	file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\8K2NEDWT\\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf},
}


@inreference{noauthor_softmax_2021,
	title = {Softmax function},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Softmax_function&oldid=1030172347},
	abstract = {The softmax function, also known as softargmax or normalized exponential function, is a generalization of the logistic function to multiple dimensions.  It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.
The softmax function takes as input a vector z of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval 
  
    
      
        [
        0
        ,
        1
        ]
      
    
    \{{\textbackslash}displaystyle [0,1]\}
  , and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. 
The standard (unit) softmax function 
  
    
      
        σ
        :
        
          
            R
          
          
            K
          
        
        →
        [
        0
        ,
        1
        
          ]
          
            K
          
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma :{\textbackslash}mathbb \{R\} {\textasciicircum}\{K\}{\textbackslash}to [0,1]{\textasciicircum}\{K\}\}
  is defined by the formula

  
    
      
        σ
        (
        
          z
        
        
          )
          
            i
          
        
        =
        
          
            
              e
              
                
                  z
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  K
                
              
              
                e
                
                  
                    z
                    
                      j
                    
                  
                
              
            
          
        
         
         
         
         
        
           for 
        
        i
        =
        1
        ,
        …
        ,
        K
        
           and 
        
        
          z
        
        =
        (
        
          z
          
            1
          
        
        ,
        …
        ,
        
          z
          
            K
          
        
        )
        ∈
        
          
            R
          
          
            K
          
        
        .
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\_\{i\}=\{{\textbackslash}frac \{e{\textasciicircum}\{z\_\{i\}\}\}\{{\textbackslash}sum \_\{j=1\}{\textasciicircum}\{K\}e{\textasciicircum}\{z\_\{j\}\}\}\}{\textbackslash} {\textbackslash} {\textbackslash} {\textbackslash} \{{\textbackslash}text\{ for \}\}i=1,{\textbackslash}dotsc ,K\{{\textbackslash}text\{ and \}\}{\textbackslash}mathbf \{z\} =(z\_\{1\},{\textbackslash}dotsc ,z\_\{K\}){\textbackslash}in {\textbackslash}mathbb \{R\} {\textasciicircum}\{K\}.\}
  In  simple words, it applies the standard exponential function to each element 
  
    
      
        
          z
          
            i
          
        
      
    
    \{{\textbackslash}displaystyle z\_\{i\}\}
   of the input vector 
  
    
      
        
          z
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash}mathbf \{z\} \}
   and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector 
  
    
      
        σ
        (
        
          z
        
        )
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\}
   is 1.
Instead of e, a different base b {\textgreater} 0 can be used.  If 0 {\textless} b {\textless} 1, smaller input components will result in larger output probabilities, and decreasing the value of b will create probability distributions that are more concentrated around the positions of the smallest input values. Conversely, if b {\textgreater} 1, larger input components will result in larger output probabilities, and increasing the value of b will create probability distributions that are more concentrated around the positions of the largest input values. Writing 
  
    
      
        b
        =
        
          e
          
            β
          
        
      
    
    \{{\textbackslash}displaystyle b=e{\textasciicircum}\{{\textbackslash}beta \}\}
   or 
  
    
      
        b
        =
        
          e
          
            −
            β
          
        
      
    
    \{{\textbackslash}displaystyle b=e{\textasciicircum}\{-{\textbackslash}beta \}\}
   (for real β) yields the expressions:

  
    
      
        σ
        (
        
          z
        
        
          )
          
            i
          
        
        =
        
          
            
              e
              
                β
                
                  z
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  K
                
              
              
                e
                
                  β
                  
                    z
                    
                      j
                    
                  
                
              
            
          
        
        
           or 
        
        σ
        (
        
          z
        
        
          )
          
            i
          
        
        =
        
          
            
              e
              
                −
                β
                
                  z
                  
                    i
                  
                
              
            
            
              
                ∑
                
                  j
                  =
                  1
                
                
                  K
                
              
              
                e
                
                  −
                  β
                  
                    z
                    
                      j
                    
                  
                
              
            
          
        
        
           for 
        
        i
        =
        1
        ,
        …
        ,
        K
        .
      
    
    \{{\textbackslash}displaystyle {\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\_\{i\}=\{{\textbackslash}frac \{e{\textasciicircum}\{{\textbackslash}beta z\_\{i\}\}\}\{{\textbackslash}sum \_\{j=1\}{\textasciicircum}\{K\}e{\textasciicircum}\{{\textbackslash}beta z\_\{j\}\}\}\}\{{\textbackslash}text\{ or \}\}{\textbackslash}sigma ({\textbackslash}mathbf \{z\} )\_\{i\}=\{{\textbackslash}frac \{e{\textasciicircum}\{-{\textbackslash}beta z\_\{i\}\}\}\{{\textbackslash}sum \_\{j=1\}{\textasciicircum}\{K\}e{\textasciicircum}\{-{\textbackslash}beta z\_\{j\}\}\}\}\{{\textbackslash}text\{ for \}\}i=1,{\textbackslash}dotsc ,K.\}
  In some fields, the base is fixed, corresponding to a fixed scale, while in others the parameter β is varied.},
	booktitle = {Wikipedia},
	urldate = {2021-07-28},
	date = {2021-06-24},
	langid = {english},
	note = {Page Version {ID}: 1030172347},
	file = {Snapshot:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\ACATHUHP\\index.html:text/html},
}

@article{king_logistic_2001,
	title = {Logistic Regression in Rare Events Data},
	pages = {27},
	author = {King, Gary and Zeng, Langche},
	date = {2001},
	langid = {english},
	file = {King and Zeng - 2001 - Logistic Regression in Rare Events Data.pdf:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\H9EQ356Y\\King and Zeng - 2001 - Logistic Regression in Rare Events Data.pdf:application/pdf},
}

@online{noauthor_loss_nodate,
	title = {Loss Functions — {ML} Glossary documentation},
	url = {https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html},
	urldate = {2021-07-28},
	file = {Loss Functions — ML Glossary documentation:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\RFJLHVYY\\loss_functions.html:text/html},
}

@online{noauthor_classification_nodate,
	title = {Classification Report — Yellowbrick v1.3.post1 documentation},
	url = {https://www.scikit-yb.org/en/latest/api/classifier/classification_report.html},
	urldate = {2021-07-29},
	file = {Classification Report — Yellowbrick v1.3.post1 documentation:C\:\\Users\\Anubhav Mehra\\Zotero\\storage\\BMFRD2S2\\classification_report.html:text/html},
}